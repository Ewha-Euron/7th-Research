### Paper Details
- **Title**: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
- **Authors**:  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela
- **Conference**: Facebook AI Research, University College London, New York University
- **Year of Publication**: 2021
- **Link**: [https://arxiv.org/abs/1606.04797](https://arxiv.org/abs/2005.11401)
- **Key Focus**: This paper proposes the Retrieval-Augmented Generation (RAG) model to enhance performance in knowledge-intensive NLP tasks. RAG combines a pre-trained sequence-to-sequence model (BART) with Dense Passage Retrieval (DPR) to retrieve documents from non-parametric memory and generate factual, specific text based on them. The model achieves superior performance in tasks like open-domain QA, Jeopardy question generation, and fact verification compared to previous approaches. Additionally, RAG allows for knowledge updates by replacing its non-parametric memory without retraining. This structure enhances model interpretability, flexibility, and suitability for incorporating up-to-date information.


### Link
üìù [[ÎÖºÎ¨∏ Î¶¨Î∑∞] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://dony-archive.tistory.com/42)

