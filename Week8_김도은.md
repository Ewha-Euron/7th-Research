### Paper Details
- **Title**: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- **Authors**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
- **Conference**: NAACL 2019
- **Year of Publication**: 2019
- **Link**: https://arxiv.org/abs/1810.04805
- **Key Focus**: This paper introduces BERT (Bidirectional Encoder Representations from Transformers), a novel language representation model that pre-trains deep bidirectional representations by conditioning on both left and right contexts of a token in all layers. Unlike traditional models, which are typically unidirectional, BERT uses two primary pre-training tasks‚ÄîMasked Language Modeling (MLM) and Next Sentence Prediction (NSP)‚Äîto achieve a rich understanding of context. The result is a pre-trained model that can be fine-tuned with minimal architectural adjustments to achieve state-of-the-art performance on a range of NLP tasks, including question answering and language inference.

### Link
üìù [[ÎÖºÎ¨∏ Î¶¨Î∑∞] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://dony-archive.tistory.com/34)
